[
  {
    "resource_id": "S3-A",
    "resource_type": "S3 bucket",
    "detected_issue": "Using S3 Standard for rarely accessed archival data",
    "reasoning": "Data is accessed less than once a year while stored in the Standard storage class, which has higher $/GB than archival classes.",
    "optimization_suggestion": "Create an S3 Lifecycle rule to transition objects from Standard to S3 Glacier Deep Archive (or Glacier Flexible Retrieval if slightly more frequent access is needed). For example, transition after 0–30 days. Ensure stakeholders accept longer restore times and retrieval fees."
  },
  {
    "resource_id": "S3-B",
    "resource_type": "S3 bucket",
    "detected_issue": "Frequent daily restores from Glacier storage",
    "reasoning": "Restoring ~100 GB/day from Glacier incurs high retrieval and request fees and adds latency, which is misaligned with daily access.",
    "optimization_suggestion": "Store these objects in S3 Standard or S3 Intelligent-Tiering instead of Glacier. Intelligent-Tiering will keep recently accessed objects in the Frequent Access tier automatically; alternatively use Standard-IA or One Zone-IA if access is not truly daily and you accept retrieval fees and (for One Zone-IA) reduced resiliency."
  },
  {
    "resource_id": "S3-C",
    "resource_type": "S3 bucket",
    "detected_issue": "Unbounded accumulation of object versions",
    "reasoning": "Versioning is enabled with an average of ~1000 versions per object and no limit/expiry, driving ongoing storage growth.",
    "optimization_suggestion": "Add S3 Lifecycle rules: (1) NoncurrentVersionExpiration to delete noncurrent versions older than a set age and/or keep only the last N noncurrent versions; (2) optionally transition older noncurrent versions to cheaper classes (e.g., Glacier Deep Archive) before expiry; (3) enable ExpiredObjectDeleteMarker cleanup."
  },
  {
    "resource_id": "S3-D",
    "resource_type": "S3 bucket",
    "detected_issue": "Charges from incomplete multipart uploads",
    "reasoning": "Uncompleted multipart uploads (20 GB) remain billable until explicitly aborted.",
    "optimization_suggestion": "Configure an S3 Lifecycle rule with AbortIncompleteMultipartUpload after a short period (e.g., 7 days). Also run a one-time cleanup using AbortMultipartUpload (CLI/SDK) to remove existing orphaned parts."
  },
  {
    "resource_id": "S3-E",
    "resource_type": "S3 bucket",
    "detected_issue": "High request overhead from millions of very small objects",
    "reasoning": "10,000,000 objects with ~10 KB size and continuous uploads lead to significant PUT/LIST request costs and per-object metadata overhead.",
    "optimization_suggestion": "Aggregate small logs into larger objects before writing to S3 (e.g., buffer and batch by size/time, or use Amazon Kinesis Data Firehose with 5–128 MB buffering) and compress them. This reduces PUT and LIST operations and improves storage efficiency. If logs have a defined retention, add Lifecycle expiration and transition older data to archival classes."
  },
  {
    "resource_id": "S3-F",
    "resource_type": "S3 bucket",
    "detected_issue": "Unnecessary cross-region replication (CRR)",
    "reasoning": "Data is replicated to us-west-2 but no applications use the replica, incurring extra storage, inter-Region transfer, and replication PUT costs.",
    "optimization_suggestion": "Disable CRR or narrow the replication rule with filters to only required prefixes/tags. If replication is not needed, delete the destination data. If compliance requires replication, consider Same-Region Replication (SRR) or remove Replication Time Control where not required."
  },
  {
    "resource_id": "S3-J",
    "resource_type": "S3 bucket",
    "detected_issue": "Costly frequent full-bucket LIST operations",
    "reasoning": "Applications perform full scans, resulting in ~500 LIST operations per hour, which drives S3 request costs.",
    "optimization_suggestion": "Use S3 Inventory (daily/weekly) for full listings instead of frequent LIST calls, and/or maintain an object index via S3 Event Notifications (PUT/DELETE) to Lambda -> DynamoDB. When listing is necessary, use ListObjectsV2 with prefixes and continuation tokens to scope and reduce scans."
  }
]